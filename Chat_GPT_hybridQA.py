from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import OpenAI
from dotenv import load_dotenv
import os

# ğŸ” API í‚¤
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# ëª¨ë¸ ì„¤ì •
llm = OpenAI(openai_api_key=api_key)
embedding = OpenAIEmbeddings(openai_api_key=api_key)

# ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ë° ë¶„í• 
loader = TextLoader("data/sample.txt", encoding='utf-8')
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)
docs = text_splitter.split_documents(documents)

# ë²¡í„° ì €ì¥ì†Œ
db = FAISS.from_documents(docs, embedding)
retriever = db.as_retriever()

# ğŸ’¬ ì§ˆì˜ ë£¨í”„
while True:
    query = input("ì„±ì£¼ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ğŸ¤–: ")
    if query.lower() in ["exit", "quit"]:
        print("AI ì¢…ë£Œí•©ë‹ˆë‹¤ ğŸ‘‹")
        break

    # 1. ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ ì°¾ê¸°
    docs = retriever.get_relevant_documents(query)

    # 2. ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    doc_context = "\n".join([doc.page_content for doc in docs]) if docs else "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ ì—†ìŒ."

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¬¸ì„œ + ì§ˆë¬¸ ê°™ì´ ì „ë‹¬)
    prompt = f"""
    ë„ˆëŠ” ë˜‘ë˜‘í•œ AIì•¼. ì•„ë˜ëŠ” ì°¸ê³  ë¬¸ì„œì´ê³ , ì•„ë˜ ì§ˆë¬¸ì— ë‹µí•´ì•¼ í•´.
    ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì—†ìœ¼ë©´ ë„¤ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ëŒ€ë‹µí•´ì¤˜.

    [ë¬¸ì„œ ë‚´ìš©]
    {doc_context}

    [ì§ˆë¬¸]
    {query}

    [ë‹µë³€]
    """

    # 4. GPTì— ì „ë‹¬
    answer = llm(prompt)
    print("ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ AI:", answer)
